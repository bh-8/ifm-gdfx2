\documentclass{article}

\usepackage[german]{babel}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{multirow}
\usepackage{scrextend}
\usepackage{tabularx}
\usepackage{arydshln}
\usepackage{xspace}
\usepackage{xurl}

\deffootnote{0em}{1.6em}{\thefootnotemark.\enskip}
\def\arraystretch{1.2}

\newcommand{\imgsize}{\texttt{(224, 224, 3)}\xspace}
\newcommand{\resnet}{ResNet50\xspace}
\newcommand{\effnet}{EfficientNetB0\xspace}

\title{Generalizable Deepfake Detection}
\author{Bernhard Birnbaum}

\begin{document}
    \maketitle

    \section{Motivation \& Stand der Technik}
    Im Rahmen des Praktikums \enquote{Implementierung in Forensik und Mediensicherheit} soll ein Framework zur Detektion von Deepfakes (Bildsequenzen) implementiert werden.
    Dabei wird neben der Detektion eine Klassifikation verschiedener Deepfake-Techniken, insbesondere \enquote{Face-Swap} und \enquote{Face-Reenactment}, etabliert.
    Im Zentrum des Projekts steht die Fragestellung, inwiefern ein geeignetes Machine-Learning-Modell~\footnote{Machine Learning, Google for Developers, \url{https://developers.google.com/machine-learning}} konzeptioniert, implementiert bzw. optimiert werden kann, das diesen Ansprüchen genügt.
    Dazu wird zunächst ein Baseline-Modell trainiert, welches im weiteren Verlauf der Untersuchung mit verschiedenen Komponenten angepasst werden kann.
    Abschließend sollen die Auswirkungen auf die Modell-Performance anhand geeigneter Metriken evaluiert werden.
    Zur Implementierung der Deepfake-Klassifikation werden Deeplearning-Methoden~\cite{deeplearningbook} eingesetzt.
    \subsection{Architekturen für sequentielle Daten}
    \begin{itemize}
        \item RNNs - LSTMs - BiLSTMs % https://medium.com/@anishnama20/understanding-bidirectional-lstm-for-sequential-data-processing-b83d6283befc
    \end{itemize}
    \subsection{Merkmalsextraktoren als Baseline-Modelle}
    \subsubsection{\resnet}
    \resnet ist eine CNN-Architektur (\enquote{Convolutional Neural Networks}) für tiefe neuronale Netzwerke, die zu den \enquote{Residual Networks}~\cite{resnet} gehört und eine Tiefe von 50 Schichten aufweist.
    ResNet-Modelle sind besonders gut für die Klassifikation von Bildern geeignet und erzielen dabei State-of-the-Art-Ergebnisse, weshalb sie auch häufig als Merkmalsextraktoren im Bereich des maschinellen Sehens eingesetzt werden~\footnote{Exploring ResNet50: An In-Depth Look at the Model Architecture and Code Implementation, \url{https://medium.com/@nitishkundu1993/exploring-resnet50-an-in-depth-look-at-the-model-architecture-and-code-implementation-d8d8fa67e46f}}.
    Um das Problem der Vanishing-Gradients zu lösen, also dass die Gradienten in den tieferen Schichten verschwindend gering werden, nutzt ResNet sogenannte \enquote{Residual Connections}, wodurch die Eingabedaten eine oder mehrere Schichten im Netzwerk \enquote{überspringen} können.
    \subsubsection{\effnet}
    \effnet ist neben \resnet eine alternative CNN-Architektur und gehört zur EfficientNet-Familie~\cite{efficientnet}.
    Der Hauptunterschied zu ResNet besteht in der Skalierung: bei EfficientNet können nicht nur weitere Schichten hinzugefügt, sondern das Netz kann in Tiefe, Breite und Auflösung angepasst werden (\enquote{Compound Model Scaling})~\footnote{Understanding EfficientNet - The most powerful CNN architecture, \url{https://arjun-sarkar786.medium.com/understanding-efficientnet-the-most-powerful-cnn-architecture-eaeb40386fad}}.
    Desweiteren nutzt EfficientNet sogenannte \enquote{MBConv-Blöcke}, eine effizientere Strategie zur Implementierung von \enquote{Inverted Residual Blocks}.
    Dadurch verringert sich die Parameteranzahl im Vergleich zu ResNet erheblich, was zu einem deutlich reduzierten Rechenaufwand bei ählicher Genauigkeit führt.
    \subsection{DF40-Datenset}
    Um einen Klassifikator zu trainieren, der neben der Detektion zusätzlich verschiedene Arten von Deepfakes erkennen soll, ist ein breit gestreuter Trainings- bzw. Validierungsdatensatz notwendig.
    Dafür wurde das DF40-Datenset~\cite{yan2024df40} ausgewählt, eine Sammlung von Deepfake-Erzeugnissen von 40 verschiedenen Tools inklusive der Originaldaten.
    Im Rahmen dieser Arbeit soll in einem ersten Proof-of-Concept zunächst zwischen den Klassen \enquote{original}, \enquote{faceswap} (10 Tools) und \enquote{facereenactment} (13 Tools) unterschieden werden.
    \begin{lstlisting}[language=Python,caption={Ordnerstruktur des DF40-Datensets im \enquote{io}-Verzeichnis}]
df40/
    test/
        original/
            ffpp/frames/
                <item>/*.png
        face_swap/
            <tool>/frames/
                <item>/*.png
        face_reenact/
            <tool>/frames/
                <item>/*.png
    train/
        original/
            ffpp/frames/
                <item>/*.png
        face_swap/
            <tool>/frames/
                <item>/*.png
        face_reenact/
            <tool>/frames/
                <item>/*.png
    \end{lstlisting}
    Da das DF40-Datenset bereits als vorverarbeitete Version angeboten wird, müssen keine erweiterten vorbereitenden Schritte wie FaceDetection/Cropping ausgeführt werden.
    Alle Frames im Datenset haben dieselben Abmessungen (\texttt{256x256x3}), die Sequenzen variieren allerdings in ihrer Länge zwischen 8 und 32 (Anzahl \texttt{*.png}-Bilder pro Item).

    \section{Konzept}
    \subsection{Vorverarbeitung des Datensets}
    \subsubsection{Sequenzlängen-Filterung}
    Dadurch, dass die Sequenzlängen im Datenset variieren, muss bereits im Vorfeld festgelegt werden, wie viele Frames das Modell pro Item verarbeiten soll.
    Basierend auf der Sequenzlänge werden alle Sequenzen entweder zugeschnitten (sollten sie zu lang sein) oder aus dem Datenset entfernt (sollten sie zu kurz sein).
    Alternativ könnten zu kurze Sequenzen auch mit Padding aufgefüllt werden.
    Da das Datenset allerdings groß genug ist und nicht allzu viele Sequenzen betroffen sind, sowie zusätzlich unbalancierte Klassen mit Hilfe von gewichteten Klassen ausgeglichen werden, wurde darauf verzichtet.
    \subsubsection{Instanzen pro Klasse}
    Je nach Sequenzlänge ergeben sich somit die folgenden Anzahlen von Instanzen pro Klasse, wobei der Split (Einteilung Trainings- und Testdaten) vom Datensatz vorgegeben wird:
    \begin{table}[!h]
        \centering
        \caption{Anzahl Instanzen pro Klasse im Trainings- und Testdatensatz}
        \begin{tabularx}{\textwidth}{|X||c|c|c||c|c|c|}
            \hline
            \multirow{2}{*}{\textbf{Sequenzlänge}} & \multicolumn{3}{c||}{\textbf{Trainingsdaten}} & \multicolumn{3}{c|}{\textbf{Testdaten}} \\\cdashline{2-7}
            & \texttt{OR} & \texttt{FS} & \texttt{FR} & \texttt{OR} & \texttt{FS} & \texttt{FR} \\
            \hline
            8 Frames  &  &  &  &  &  &  \\
            \hline
            12 Frames & 999 & 6509 & 8460 & 999 & 7070 & 9405 \\
            \hline
            16 Frames &  &  &  &  &  &  \\
            \hline
        \end{tabularx}
    \end{table}
    \begin{itemize}
        \item Preprocessing/Normalisierung/Mischen
    \end{itemize}
    \subsection{Aufbau des Modells}
    Das in dieser Arbeit entwickelte Modell besteht aus den folgenden Schichten:
    \begin{enumerate}
        \item\textbf{Input-Layer}:
            In der Eingabeschicht wird die Form des Eingabetensors definiert.
            Diese ergibt sich zum einen aus den Abmessungen eines einzelnen Frames \imgsize, zum anderen aus der Länge der Sequenz, mit der das Modell arbeiten soll.
            \\\textbf{Beispiel} für valide Form einer Eingabe (Sequenzlänge 12): \texttt{(12, 224, 224, 3)}
        \item\textbf{TimeDistributed-Layer}:
            Diese Schicht implementiert eine Merkmalsextraktion für jeden Frame in der Sequenz, wahlweise durch \resnet oder \effnet.
            Die Gewichte dieser Schicht werden initial mit vortrainierten Gewichten via \enquote{\texttt{imagenet}}~\footnote{ImageNet, \url{https://www.image-net.org/}} belegt.
            Die Aufgabe der Merkmalsextraktion ist, die Eingangsgröße durch Reduktion mit Faltungsschichten und Pooling-Operationen schrittweise zu verkleinern.
            Für einen Frame \imgsize entsteht dadurch eine Feature-Map der Form \texttt{(7, 7, 2048)} bei \resnet bzw. \texttt{(7, 7, 1280)} bei \effnet.
            Die Merkmale aus der letzten Faltungsschicht werden mit einem \enquote{GlobalAveragePooling2D} zusammengefasst.
            Somit kann ein Frame an dieser Stelle durch einen Tensor der Form \texttt{(2048)} für \resnet bzw. \texttt{(1280)} für \effnet repräsentiert werden, welcher jeweils die extrahierten Merkmale eines Einzelbildes beinhaltet.
            \\\textbf{Beispiel} für valide Form einer Ausgabe (Sequenzlänge 12, \resnet): \texttt{(12, 2048)}
            \\\textbf{Beispiel} für valide Form einer Ausgabe (Sequenzlänge 12, \effnet): \texttt{(12, 1280)}
    \end{enumerate}
    \begin{itemize}
        \item BiLSTM/Layer
        \item DropOut-Layer
        \item Dense-Layer (Softmax, L2-Regularisierung, Kernel/Bias)
    \end{itemize}
    \subsection{Training}
    \begin{itemize}
        \item Optimizer (adam)
        \item Minimierung Loss Funktion (categorical crossentropy)
        \item LR-Scheduler (ReduceLROnPlateau)
        \item Epochen/Early-Stopping
    \end{itemize}
    \subsection{Validation}
    \begin{itemize}
        \item categorical accuracy
        \item f1 score
        \item area under curve
        \item Detektionszeit
    \end{itemize}

    \section{Implementierung}
    \begin{itemize}
        \item Entwicklungsumgebung/Cluster: MiniForge
        \item Python- und Tool-Versionen, Bibliotheken, ... % https://www.tensorflow.org/guide/keras/training_with_built_in_methods
    \end{itemize}
    \subsection{PyTorch vs. Tensorflow}
    \begin{itemize}
        \item Vergleich und Abwägung
    \end{itemize}
    \section{Evaluation}
    \begin{itemize}
        \item Model Performance anhand von Metriken
    \end{itemize}

    \section{Zusammenfassung}
    \subsection{Fazit}
    \subsection{Ausblick für zukünftige Arbeiten}
    \begin{itemize}
        \item DataAugmentation
        \item zusätzlicher Validierungsdatensatz % https://huggingface.co/datasets/faridlab/deepspeak_v1
        \item Weitere Klassen einfügen (über FS und FR hinaus)
        \item Weitere Optimierungsmöglichkeiten
        \item ViT
        \item Vorverarbeitungsmethoden (Frames aus Video extrahieren und FaceCropping)
        \item Auflösung erhöhen: Merkmalsextraktoren, Input-Size, ...
    \end{itemize}

    \bibliographystyle{plain}
    \bibliography{refs}
\end{document}
