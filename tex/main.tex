\documentclass{article}

\usepackage{amsmath}
\usepackage[german]{babel}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{lmodern}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{scrextend}
\usepackage{tabularx}
\usepackage{arydshln}
\usepackage{xspace}
\usepackage{xurl}

\deffootnote{0em}{1.6em}{\thefootnotemark.\enskip}
\def\arraystretch{1.2}

\newcommand{\imgsize}{\texttt{(224, 224, 3)}\xspace}
\newcommand{\resnet}{ResNet50\xspace}
\newcommand{\effnet}{EfficientNetB0\xspace}

\title{Generalizable Deepfake Detection}
\author{Bernhard Birnbaum}

\begin{document}
    \maketitle

    \section{Motivation \& Stand der Technik}
    Im Rahmen des Praktikums \enquote{Implementierung in Forensik und Mediensicherheit} soll ein Framework zur Detektion von Deepfakes (Bildsequenzen) implementiert werden.
    Dabei wird neben der Detektion eine Klassifikation verschiedener Deepfake-Techniken, insbesondere \enquote{Face-Swap} und \enquote{Face-Reenactment}, etabliert.
    Im Zentrum des Projekts steht die Fragestellung, inwiefern ein geeignetes Machine-Learning-Modell~\footnote{Machine Learning, Google for Developers, \url{https://developers.google.com/machine-learning}} konzeptioniert, implementiert bzw. optimiert werden kann, das diesen Ansprüchen genügt.
    Dazu wird zunächst ein Baseline-Modell trainiert, welches im weiteren Verlauf der Untersuchung mit verschiedenen Komponenten angepasst werden kann.
    Abschließend sollen die Auswirkungen auf die Modell-Performance anhand geeigneter Metriken evaluiert werden.
    Zur Implementierung der Deepfake-Klassifikation werden Deeplearning-Methoden~\cite{deeplearningbook} eingesetzt.
    \subsection{Architekturen für sequentielle Daten}
    \begin{itemize}
        \item RNNs - LSTMs - BiLSTMs % https://medium.com/@anishnama20/understanding-bidirectional-lstm-for-sequential-data-processing-b83d6283befc
        \item Ein LSTM-Netzwerk ist speziell darauf ausgelegt, sequentielle Abhängigkeiten zu erfassen. Es kann Informationen über vergangene Zeitpunkte behalten und damit zeitliche Muster in der Sequenz analysieren. Der hier verwendete BiLSTM-Layer erweitert diese Fähigkeit, indem er die Sequenz in beide Richtungen verarbeitet – sowohl vorwärts als auch rückwärts. Dies ermöglicht es dem Modell, Abhängigkeiten sowohl aus der Vergangenheit als auch aus der Zukunft eines Frames zu berücksichtigen.
    \end{itemize}
    \subsection{Merkmalsextraktoren als Baseline-Modelle}
    \subsubsection{\resnet}
    \resnet ist eine CNN-Architektur (\enquote{Convolutional Neural Networks}) für tiefe neuronale Netzwerke, die zu den \enquote{Residual Networks}~\cite{resnet} gehört und eine Tiefe von 50 Schichten aufweist.
    ResNet-Modelle sind besonders gut für die Klassifikation von Bildern geeignet und erzielen dabei State-of-the-Art-Ergebnisse, weshalb sie auch häufig als Merkmalsextraktoren im Bereich des maschinellen Sehens eingesetzt werden~\footnote{Exploring ResNet50: An In-Depth Look at the Model Architecture and Code Implementation, \url{https://medium.com/@nitishkundu1993/exploring-resnet50-an-in-depth-look-at-the-model-architecture-and-code-implementation-d8d8fa67e46f}}.
    Um das Problem der Vanishing-Gradients zu lösen, also dass die Gradienten in den tieferen Schichten verschwindend gering werden, nutzt ResNet sogenannte \enquote{Residual Connections}, wodurch die Eingabedaten eine oder mehrere Schichten im Netzwerk \enquote{überspringen} können.
    \subsubsection{\effnet}
    \effnet ist neben \resnet eine alternative CNN-Architektur und gehört zur EfficientNet-Familie~\cite{efficientnet}.
    Der Hauptunterschied zu ResNet besteht in der Skalierung: bei EfficientNet können nicht nur weitere Schichten hinzugefügt, sondern das Netz kann in Tiefe, Breite und Auflösung angepasst werden (\enquote{Compound Model Scaling})~\footnote{Understanding EfficientNet - The most powerful CNN architecture, \url{https://arjun-sarkar786.medium.com/understanding-efficientnet-the-most-powerful-cnn-architecture-eaeb40386fad}}.
    Desweiteren nutzt EfficientNet sogenannte \enquote{MBConv-Blöcke}, eine effizientere Strategie zur Implementierung von \enquote{Inverted Residual Blocks}.
    Dadurch verringert sich die Parameteranzahl im Vergleich zu ResNet erheblich, was zu einem deutlich reduzierten Rechenaufwand bei ählicher Genauigkeit führt.
    \subsection{DF40-Datenset}
    Um einen Klassifikator zu trainieren, der neben der Detektion zusätzlich verschiedene Arten von Deepfakes erkennen soll, ist ein breit gestreuter Trainings- bzw. Validierungsdatensatz notwendig.
    Dafür wurde das DF40-Datenset~\cite{yan2024df40} ausgewählt, eine Sammlung von Deepfake-Erzeugnissen von 40 verschiedenen Tools inklusive der Originaldaten.
    Im Rahmen dieser Arbeit soll in einem ersten Proof-of-Concept zunächst zwischen den Klassen \enquote{original}, \enquote{faceswap} (10 Tools) und \enquote{facereenactment} (13 Tools) unterschieden werden.
    \begin{lstlisting}[language=Python,caption={Ordnerstruktur des DF40-Datensets im \enquote{io}-Verzeichnis}]
df40/
    test/
        original/
            ffpp/frames/
                <item>/*.png
        face_swap/
            <tool>/frames/
                <item>/*.png
        face_reenact/
            <tool>/frames/
                <item>/*.png
    train/
        original/
            ffpp/frames/
                <item>/*.png
        face_swap/
            <tool>/frames/
                <item>/*.png
        face_reenact/
            <tool>/frames/
                <item>/*.png
    \end{lstlisting}
    Da das DF40-Datenset bereits als vorverarbeitete Version angeboten wird, müssen keine erweiterten vorbereitenden Schritte wie FaceDetection/Cropping ausgeführt werden.
    Alle Frames im Datenset haben dieselben Abmessungen (\texttt{256x256x3}), die Sequenzen variieren allerdings in ihrer Länge zwischen 8 und 32 (Anzahl \texttt{*.png}-Bilder pro Item).

    \section{Konzept}
    \subsection{Vorverarbeitung des Datensets}
    \subsubsection{Sequenzlängen-Filterung}
    Dadurch, dass die Sequenzlängen im Datenset variieren, muss bereits im Vorfeld festgelegt werden, wie viele Frames das Modell pro Item verarbeiten soll.
    Basierend auf der Sequenzlänge werden alle Sequenzen entweder zugeschnitten (sollten sie zu lang sein) oder aus dem Datenset entfernt (sollten sie zu kurz sein).
    Alternativ könnten zu kurze Sequenzen auch mit Padding aufgefüllt werden.
    Da das Datenset allerdings groß genug ist und nicht allzu viele Sequenzen betroffen sind, sowie zusätzlich unbalancierte Klassen mit Hilfe von gewichteten Klassen ausgeglichen werden, wurde darauf verzichtet.
    \subsubsection{Instanzen pro Klasse}
    Je nach Sequenzlänge ergeben sich somit die folgenden Anzahlen von Instanzen pro Klasse, wobei der Split (Einteilung Trainings- und Testdaten) vom Datensatz vorgegeben wird:
    \begin{table}[!h]
        \centering
        \caption{Anzahl Instanzen pro Klasse im Trainings- und Testdatensatz}
        \begin{tabularx}{\textwidth}{|X||c|c|c||c|c|c|}
            \hline
            \multirow{2}{*}{\textbf{Sequenzlänge}} & \multicolumn{3}{c||}{\textbf{Trainingsdaten}} & \multicolumn{3}{c|}{\textbf{Testdaten}} \\\cdashline{2-7}
            & \texttt{OR} & \texttt{FS} & \texttt{FR} & \texttt{OR} & \texttt{FS} & \texttt{FR} \\\hline\hline
            8 Frames  &  &  &  &  &  &  \\\hline
            12 Frames & 999 & 6509 & 8460 & 999 & 7070 & 9405 \\\hline
            16 Frames &  &  &  &  &  &  \\\hline
        \end{tabularx}
    \end{table}
    \begin{itemize}
        \item Preprocessing/Normalisierung/Mischen
    \end{itemize}
    \subsection{Aufbau des Modells}
    Das in dieser Arbeit entwickelte Modell implementiert eine Klassifikation von Bildsequenzen (mit fixer Länge und Auflösung) in 3 Klassen.
    Die einzelnen Schichten sind dabei folgendermaßen charakterisiert:
    \begin{enumerate}
        \item\textbf{Input-Layer}:
            In der Eingabeschicht wird die Form des Eingabetensors definiert.
            Diese ergibt sich zum einen aus den Abmessungen eines einzelnen Frames \imgsize, zum anderen aus der Länge der Sequenz, mit der das Modell arbeiten soll.
            \\\textbf{Beispiel} für valide Form einer Eingabe (Sequenzlänge 12): \texttt{(12, 224, 224, 3)}
        \item\textbf{TimeDistributed-Wrapper mit Merkmalsextraktion}:
            Diese Schicht implementiert eine Merkmalsextraktion der zu klassifizierenden Bildsequenz, wahlweise durch \resnet oder \effnet.
            Dabei ist das Ziel, die Eingangsgröße eines jeden Einzelbildes durch Reduktion mit Faltungsschichten und Pooling-Operationen schrittweise zu verkleinern.
            Für jeden Frame \imgsize entsteht dadurch eine Feature-Map der Form \texttt{(7, 7, 2048)} bei \resnet bzw. \texttt{(7, 7, 1280)} bei \effnet.
            Die Merkmale aus der letzten Faltung des Netzwerks werden abschließend mit einem GlobalAveragePooling2D zusammengefasst, wodurch jeder Frame der Sequenz auf einen Tensor der Form \texttt{(2048)} für \resnet bzw. \texttt{(1280)} für \effnet reduziert wird.
            Da die Merkmalsextraktion auf jeden Frame der zu klassifizierenden Bildsequenz parallel angewandt werden muss, wird die Schicht in einem sogenannten TimeDistributed-Wrapper implementiert.
            \\\textbf{Beispiel} für valide Form einer Ausgabe (Sequenzlänge 12, \resnet): \texttt{(12, 2048)}
            \\\textbf{Beispiel} für valide Form einer Ausgabe (Sequenzlänge 12, \effnet): \texttt{(12, 1280)}
        \item\textbf{Bidirectional-Wrapper mit LSTM}:
            Die Sequenz von Feature-Maps wird in der nächsten Schicht mit einem LSTM verarbeitet.
            Da das LSTM aus 256 Neuronen besteht und mit Hilfe des Bidirectional-Wrappers in beide Richtungen trainiert wird, werden insgesamt 512 Merkmalsdimensionenen erfasst.
            Desweiteren werden mehrere Maßnahmen gegen Overfitting in das Modell etabliert:
            Zum einen wird auf die Eingaben und rekurrenten Verbindungen der LSTM-Zellen ein Dropout eingeführt, zum anderen wird auf die Gewichte der Eingangsverbindungen und rekurrenten Verbindungen eine L2-Regularisierung angewandt.
            Die Ausgabe des BiLSTM enthält eine verdichtete Darstellung der gesamten Bildsequenz, einschließlich relevanter zeitlicher Abhängigkeiten. 
            \\\textbf{Beispiel} für valide Form einer Ausgabe: \texttt{(512)}
        \item\textbf{Dropout-Layer}:
            Nach der Sequenzverarbeitung wird eine weitere Dropout-Schicht auf die gesamte Ausgabe des BiLSTMs angewandt.
        \item\textbf{Dense-Layer}:
            In einer abschließenden vollständig verbundenen Schicht werden die vom BiLSTM extrahierten Merkmale in die Klassifikationswerte umgewandelt.
            Dabei werden besonders große Gewichte bestraft, indem eine weitere L2-Regularisierung zum Einsatz kommt.
            Für jede Klasse gibt es ein Neuron in der Ausgabeschicht.
            Als Aktivierungsfunktion wird \texttt{softmax} genutzt, welche die Rohwerte in Wahrscheinlichkeiten für die Klassen umwandelt, sodass die Summe über alle Neurone der Ausgabe 1 ergibt.
            \\\textbf{Beispiel} für valide Form einer Ausgabe: \texttt{(3)}
    \end{enumerate}
    \begin{table}[!h]
        \centering
        \caption{Modellkonfigurationen}
        \begin{tabularx}{\textwidth}{|X|X||c|c|}
            \hline
            \textbf{Sequenzlänge} & \textbf{Batch Size} & \textbf{\resnet} & \textbf{\effnet} \\\hline\hline
            8 Frames & 12er Batches & X & X \\\hline
            12 Frames & 8er Batches & X & X \\\hline
            16 Frames & 6er Batches & X & X \\\hline
        \end{tabularx}
    \end{table}
    \subsection{Training}
    \begin{itemize}
        \item\textbf{Optimierer}:
            \\Zum Trainieren wird der Optimizer AdamW verwendet, eine um Weight-Decay erweiterte Variante des Adam-Optimizers (Adaptive Moment Estimation mit L2-Regularisierung).
            AdamW verwaltet für jede Gewichtskomponente eine eigene Lernrate, die dynamisch angepasst wird, wodurch auf einen zusätzlichen LR-Scheduler verzichtet werden kann.
            % Desweiteren werden vergangene Gradienten mit aktuellen Gradienten kombiniert, um stabilere Updates zu ermöglichen. 
        \item\textbf{Verlustfunktion}:
            \\Als zu minimierende Verlustfunktion wird CategoricalCrossEntropyLoss verwendet:
            \\$\displaystyle L=-\sum_{i=1}^Cy_i\log(\hat y_i)$ mit Anzahl Klassen $C$, Grundwahrheit $y_i$ und Vorhersage $\hat y_i$
        \item\textbf{Merkmalsextraktoren}:
            \\Dem ausgewählten Merkmalsextraktor (\resnet oder \effnet) werden initial vortrainierte Gewichte via \texttt{imagenet}~\footnote{ImageNet, \url{https://www.image-net.org/}} zugewiesen.
            In den ersten 3 Epochen des Trainings wird er einem Fine-Tuning unterzogen.
            Die Anzahl von Schichten, deren Gewichte für das Training freigegeben werden sollen, wird mit folgender Formel berechnet (für Epoche $i$ und Tiefe des Merkmalsextraktors $d$):
            \\\[
                u_i= 
            \begin{dcases}
                \Big\lceil\frac{d}{2^{(2+i)}}\Big\rceil & \text{if } 0\le i \le 2\\
                0 & \text{otherwise}
            \end{dcases}
            \]
            \\Die Größe der Lernrate wird basierend auf $\lambda=0.001$ durch:
            \\\[
                \lambda_i= 
            \begin{dcases}
                \frac{\lambda}{10\cdot 2^i} & \text{if } 0\le i \le 2\\
                \max(\frac{\lambda}{2^i}, 1\cdot 10^{-5}) & \text{otherwise}
            \end{dcases}
            \]
            \begin{table}[!h]
                \centering
                \caption{Keras-Schichten für Fine-Tuning der Merkmalsextraktoren und Lernrate pro Epoche}
                \begin{tabularx}{\textwidth}{|X||c|c||c|}
                    \hline
                    \textbf{Epoche} & \textbf{\resnet ($d=176$)} & \textbf{\effnet ($d=239$)} & \textbf{Lernrate ($\lambda=0.001$)} \\\hline\hline
                    Epoche 1 & $u_0=44$ (25\%) & $u_0=60$ (25\%) & $\lambda_0=1\cdot 10^{-4}$ \\\hline
                    Epoche 2 & $u_1=22$ (12.5\%) & $u_1=30$ (12.5\%) & $\lambda_1=5\cdot 10^{-5}$ \\\hline
                    Epoche 3 & $u_2=11$ (6.25\%) & $u_2=15$ (6.25\%) & $\lambda_2=2.5\cdot 10^{-5}$ \\\hline
                    Epoche 4 & \multirow{6}{*}{n.a.} & \multirow{6}{*}{n.a.} & $\lambda_3=1.25\cdot 10^{-4}$ \\\cline{1-1}\cline{4-4}
                    Epoche 5 &  &  & $\lambda_4=6.25\cdot 10^{-5}$ \\\cline{1-1}\cline{4-4}
                    Epoche 6 &  &  & $\lambda_5=3.125\cdot 10^{-5}$ \\\cline{1-1}\cline{4-4}
                    Epoche 7 &  &  & $\lambda_6=1.5625\cdot 10^{-5}$ \\\cline{1-1}\cline{4-4}
                    Epoche 8 &  &  & $\lambda_7=1\cdot 10^{-5}$ \\\cline{1-1}\cline{4-4}
                    Epoche 9 &  &  & $\lambda_8=1\cdot 10^{-5}$ \\\hline
                \end{tabularx}
            \end{table}
        \item Early-Stopping oder fixe Epochen?
    \end{itemize}
    \subsection{Validation}
    \begin{itemize}
        \item categorical accuracy
        \item f1 score
        \item precision
        \item recall
        \item Detektionszeit
        \item warum kein area under curve
    \end{itemize}

    \section{Implementierung}
    \begin{itemize}
        \item Entwicklungsumgebung/Cluster: MiniForge
        \item Python- und Tool-Versionen, Bibliotheken, ... % https://www.tensorflow.org/guide/keras/training_with_built_in_methods
    \end{itemize}
    \subsection{PyTorch vs. Tensorflow}
    \begin{itemize}
        \item Vergleich und Abwägung
    \end{itemize}
    \section{Evaluation}
    \begin{itemize}
        \item Model Performance anhand von Metriken
    \end{itemize}

    \section{Zusammenfassung}
    \subsection{Fazit}
    \subsection{Ausblick für zukünftige Arbeiten}
    \begin{itemize}
        \item DataAugmentation
        \item zusätzlicher Validierungsdatensatz % https://huggingface.co/datasets/faridlab/deepspeak_v1
        \item Weitere Klassen einfügen (über FS und FR hinaus)
        \item Weitere Optimierungsmöglichkeiten
        \item ViT
        \item Vorverarbeitungsmethoden (Frames aus Video extrahieren und FaceCropping)
        \item Auflösung erhöhen: Merkmalsextraktoren, Input-Size, ...
    \end{itemize}

    \bibliographystyle{plain}
    \bibliography{refs}
\end{document}
